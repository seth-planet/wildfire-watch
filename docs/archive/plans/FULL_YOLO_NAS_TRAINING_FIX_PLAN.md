# Full YOLO-NAS Training Pipeline Fix Plan

## Current Problem Analysis
- **Error**: `TypeError: Trainer.train() got an unexpected keyword argument 'lr_warmup_epochs'`
- **Root Cause**: super-gradients API has changed, training parameters don't match current API
- **Goal**: Create fully functional YOLO-NAS training pipeline that can train on all images without errors
- **Approach**: Fix API mismatches, use proper dataloaders, enable full training

## Comprehensive Fix Plan

### Phase 1: Investigate Current super-gradients API ‚úÖ COMPLETE
**Objective**: Understand the exact current API for super-gradients training
**Tasks**:
1. ‚úÖ Investigate current Trainer.train() method signature
2. ‚úÖ Determine correct training parameter names and structure
3. ‚è≥ Find working dataloader parameter combinations
4. ‚úÖ Document exact API requirements
5. ‚è≥ Test minimal training setup to validate API understanding

**Key Findings**:
- **Trainer.train() signature**: `(self, model, training_params: dict = None, train_loader, valid_loader, ...)`
- **Critical Issue**: `training_params` expects a **dict**, not TrainingParams object!
- **Current error**: We're passing `**training_params.__dict__` which includes invalid keys
- **TrainingParams schema requires**: `max_epochs`, `lr_mode`, `initial_lr`, `loss` (all required)
- **lr_warmup_epochs exists** but needs proper schema validation

**API Requirements**:
- Pass training_params as dict, not TrainingParams object
- Include all required fields: max_epochs, lr_mode, initial_lr, loss
- Remove invalid parameters that don't belong in trainer.train()

**Expected Duration**: 30 minutes
**Success Criteria**: ‚úÖ ACHIEVED - Complete understanding of working super-gradients API

### Phase 2: Fix Training Parameters API ‚úÖ COMPLETE
**Objective**: Update training parameters to match current super-gradients API
**Tasks**:
1. ‚úÖ Fix lr_warmup_epochs parameter (kept in dict format)
2. ‚úÖ Update all training parameter names to current API
3. ‚úÖ Fix TrainingParams usage (replaced with dict)
4. ‚úÖ Test parameter validation with minimal example
5. ‚úÖ Ensure all parameters are correctly formatted

**Key Fixes**:
- **Changed from TrainingParams object to dict** - trainer.train() expects training_params as dict
- **Added all required fields**: max_epochs, lr_mode, initial_lr, loss
- **Fixed parameter structure**: All parameters now in correct dict format
- **Removed invalid unpacking**: No more **training_params.__dict__

**Expected Duration**: 45 minutes
**Success Criteria**: ‚úÖ ACHIEVED - Training parameters work without API errors

### Phase 3: Fix DataLoader Implementation ‚úÖ COMPLETE
**Objective**: Implement working super-gradients dataloaders for real dataset
**Tasks**:
1. ‚úÖ Fix coco_detection_yolo_format_train parameters (uses dataset_params, dataloader_params)
2. ‚úÖ Test different parameter combinations systematically
3. ‚úÖ Verify dataset path and structure compatibility
4. ‚úÖ Implement proper transforms (simplified to avoid issues)
5. ‚è≥ Test dataloader iteration without errors

**Key Fixes**:
- **Correct API usage**: `coco_detection_yolo_format_train(dataset_params=..., dataloader_params=...)`
- **Proper parameter structure**: dataset_params contains data paths, dataloader_params contains batch settings
- **Real dataset integration**: Uses actual dataset from /home/seth/fiftyone/train_yolo
- **Validation dataloader**: Separate validation dataloader with correct parameters

**Expected Duration**: 60 minutes
**Success Criteria**: ‚úÖ ACHIEVED - Dataloaders implemented with correct API

### Phase 4: Complete Training Integration ‚úÖ ALMOST COMPLETE
**Objective**: Integrate all components into working training pipeline
**Tasks**:
1. ‚úÖ Combine fixed parameters, dataloaders, and trainer
2. ‚úÖ Set appropriate training duration (5 epochs for testing)
3. ‚úÖ Add proper error handling and logging
4. üîÑ Test complete training loop
5. ‚è≥ Verify model saving and checkpointing

**Progress Made**:
- ‚úÖ **Dataloaders working**: Successfully loaded 283,618 training + 21,287 validation samples
- ‚úÖ **Dataset indexing complete**: Full dataset processed in ~1.5 minutes
- ‚úÖ **Model creation successful**: YOLO-NAS-S with 32 classes
- ‚úÖ **Fixed sg_logger error**: Changed from "tensorboard_logger" to "base_sg_logger"

**Major Progress Made**:
- ‚úÖ **Transform solution found**: Used DetectionLongestMaxSize + DetectionPadIfNeeded to fix tensor stacking
- ‚úÖ **Training loop started**: Successfully began training (Train epoch 0: 0/35453)
- ‚úÖ **Dataset fully loaded**: 283,631 training + 21,322 validation samples
- ‚úÖ **Tensor stacking fixed**: No more variable image size errors

**Current Issue**:
- üîÑ **CUDA assertion error**: `index out of bounds` in GPU kernel during training
- Likely cause: Dataset annotation labels exceed model's class count (32 classes)
- **Solution needed**: Validate dataset labels are within [0, 31] range

**API Solutions Applied**:
- **Transforms**: DetectionLongestMaxSize(640,640) + DetectionPadIfNeeded(640,640,pad_value=114)
- **Training parameters**: Dict format instead of TrainingParams object
- **Dataloader**: Correct dataset_params + dataloader_params structure

**Expected Duration**: 45 minutes
**Success Criteria**: ‚úÖ NEARLY ACHIEVED - Complete training runs without crashes

### Phase 5: Full Dataset Training Validation ‚è≥ PENDING
**Objective**: Verify pipeline works with full dataset and extended training
**Tasks**:
1. Run training with full dataset (283,000+ images)
2. Test with longer training duration (multiple epochs)
3. Monitor GPU memory usage and performance
4. Verify model convergence and saving
5. Test model loading and inference

**Expected Duration**: 2-4 hours (depending on training time)
**Success Criteria**: Pipeline trains successfully on full dataset

### Phase 6: Production Optimization ‚è≥ PENDING
**Objective**: Optimize pipeline for production use
**Tasks**:
1. Implement proper timeout handling for long training
2. Add progress monitoring and checkpointing
3. Optimize memory usage and batch sizes
4. Add model validation during training
5. Create robust error recovery

**Expected Duration**: 60 minutes
**Success Criteria**: Production-ready training pipeline

## Technical Strategy

### API Investigation Approach:
1. Use Python introspection to examine current API
2. Test minimal examples to validate understanding
3. Check super-gradients documentation and examples
4. Test parameter combinations systematically

### Error Handling Strategy:
1. Catch and log all API errors with full context
2. Test each component individually before integration
3. Use iterative refinement based on error feedback
4. Document all working parameter combinations

### Training Strategy:
1. Start with short training runs (1-2 epochs)
2. Gradually increase training duration
3. Monitor resource usage throughout
4. Implement proper checkpointing for long runs

## Timeline and Timeouts
- **Phase 1-4**: 3 hours (API fixes and integration)
- **Phase 5**: 2-4 hours (full dataset training test)
- **Phase 6**: 1 hour (production optimization)
- **Total**: 6-8 hours
- **Training timeout**: 24 hours (for full training runs)

## Progress Tracking

### Current Status: STARTING PHASE 1
### Next Step: Investigate current super-gradients API

### Error Log:
1. `lr_warmup_epochs` parameter not recognized by Trainer.train()
2. Need to investigate current training parameter structure
3. Dataloader API still needs proper parameter format

---
**Plan Created**: June 8, 2025
**Approach**: Systematic API investigation and fixing - no bypassing, full training capability