# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Wildfire Watch is an automated fire detection and suppression system that runs on edge devices. It uses AI-powered cameras with multi-camera consensus to detect fires and automatically activates sprinkler systems via GPIO control.

## Tool Usage Guidelines

### Parallel Tool Execution
When performing multiple independent operations, use parallel tool calls in a single message for optimal performance:

```python
# ✅ Good: Parallel tool execution
bash_tool.run("git status")
bash_tool.run("git diff") 
read_tool.read("file1.py")
read_tool.read("file2.py")

# ❌ Bad: Sequential messages with single tool calls
```

**Benefits:** Faster execution, more efficient workflow, better performance for complex tasks

## Development Commands

### Build and Deployment
```bash
# Generate secure certificates (required for production)
./scripts/generate_certs.sh custom

# Development with hot reload
docker-compose --env-file .env.dev up

# Full deployment
docker-compose up -d

# Multi-platform builds
./scripts/build_multiplatform.sh

# Balena deployment
balena push wildfire-watch
```

### Testing
```bash
# AUTOMATIC PYTHON VERSION SELECTION (Recommended)
# Run all tests with automatic Python version selection
./scripts/run_tests_by_python_version.sh --all

# Run specific Python version tests
./scripts/run_tests_by_python_version.sh --python312  # Most tests
./scripts/run_tests_by_python_version.sh --python310  # YOLO-NAS/super-gradients  
./scripts/run_tests_by_python_version.sh --python38   # Coral TPU/TensorFlow Lite

# Run specific test with auto-detection
./scripts/run_tests_by_python_version.sh --test tests/test_detect.py

# Validate Python environment
./scripts/run_tests_by_python_version.sh --validate

# MANUAL PYTHON VERSION SELECTION (Advanced)
python3.12 -m pytest -c pytest-python312.ini
python3.10 -m pytest -c pytest-python310.ini
python3.8 -m pytest -c pytest-python38.ini

# SPECIFIC TEST CATEGORIES
python3.12 -m pytest tests/ -v -m "not slow and not infrastructure_dependent"
python3.10 -m pytest tests/ -v -m "yolo_nas"
python3.8 -m pytest tests/ -v -m "coral_tpu"

# Disable timeouts for debugging
python3.12 -m pytest tests/ -v --timeout=0
```

### Test Timeout Configuration
- **pytest-python312.ini**: 1 hour per test, 2 hour session timeout  
- **pytest-python310.ini**: 2 hours per test, 4 hour session timeout (for training tests)
- **pytest-python38.ini**: 2 hours per test, 4 hour session timeout (for model conversion)

These timeouts ensure infrastructure-heavy tests complete successfully and prevent indefinite hangs.

### Python Version
This project requires Python 3.12. All commands should use `python3.12` and `pip3.12`.

**Exceptions**:

1. **Coral TPU**: Requires Python 3.8 for `tflite_runtime` compatibility
   - Use `python3.8` instead of `python3.12`
   - Install: `python3.8 -m pip install tflite-runtime`
   - Run `./scripts/check_coral_python.py` to verify setup

2. **YOLO-NAS Training**: Requires Python 3.10 for `super-gradients` compatibility
   - Use `python3.10` instead of `python3.12`
   - Install: `python3.10 -m pip install super-gradients`
   - Training scripts: `python3.10 converted_models/train_yolo_nas.py`

### Service Management
```bash
# Individual services
docker-compose up mqtt-broker camera-detector
docker-compose logs -f fire-consensus

# Service shell access
docker exec -it camera-detector /bin/bash

# Enable debug logging
LOG_LEVEL=DEBUG docker-compose up
```

## Architecture Overview

### Microservices Communication
All services communicate via MQTT broker with the following data flow:

1. **Camera Detector** → Discovers IP cameras, publishes to `cameras/discovered`
2. **Security NVR (Frigate)** → AI detection, publishes to `frigate/*/fire` and `frigate/*/smoke`
3. **Fire Consensus** → Validates detections, publishes to `trigger/fire_detected`
4. **GPIO Trigger** → Controls pump hardware, publishes to `gpio/status`
5. **Telemetry** → Health monitoring, publishes to `telemetry/*`

### Service Dependencies
```
mqtt_broker (core)
├── camera_detector (needs MQTT)
├── security_nvr (needs MQTT + camera config)
├── fire_consensus (needs MQTT + camera data)
├── gpio_trigger (needs MQTT + consensus)
└── cam_telemetry (needs MQTT)
```

### Key Configuration Files
- `docker-compose.yml` - Service orchestration with healthchecks and dependencies
- `.env` - Environment variables for all services
- `certs/` - TLS certificates (generated by `./scripts/generate_certs.sh`)
- `frigate_config/config.yml` - Dynamically generated by camera_detector

## Development Patterns

### Adding New Camera Support
1. Modify `camera_detector/detect.py` discovery methods
2. Update camera credential handling in environment variables
3. Test RTSP stream validation
4. Ensure MAC address tracking works for persistent identification

### AI Model Integration
- Models stored in `converted_models/` with conversion script
- Supports Coral TPU (.tflite), Hailo (.hef), ONNX, TensorRT formats
- Update `security_nvr/nvr_base_config.yml` for new models
- **Model sizes**: 640x640 (optimal accuracy), 416x416 (balanced), 320x320 (edge devices)
- **Default**: 640x640 for optimal fire detection accuracy
- **Coral TPU requires Python 3.8** for tflite_runtime compatibility
- **Always use QAT (Quantization-Aware Training)** for INT8 formats when available
- Model accuracy validation ensures <2% degradation for production
- **Calibration data**: https://huggingface.co/datasets/mailseth/wildfire-watch/resolve/main/wildfire_calibration_data.tar.gz?download=true

#### Model Conversion Timeouts
- **ONNX**: ~2-5 minutes
- **TFLite**: ~15-30 minutes (INT8 quantization)
- **TensorRT**: ~30-60 minutes (engine optimization)
- **OpenVINO**: ~10-30 minutes
- **Hailo**: ~20-40 minutes (Docker compilation)

If conversions timeout:
1. Use smaller model sizes (320x320 instead of 640x640)
2. Reduce calibration dataset size
3. Run on more powerful hardware
4. Use pre-converted models when available

### GPIO Safety Systems
- All pump control in `gpio_trigger/trigger.py` uses state machine pattern
- Maximum runtime protection (default 30 minutes)
- Automatic refill calculation based on runtime
- Hardware simulation mode for development on non-Pi systems

### Multi-Camera Consensus Logic
- Located in `fire_consensus/consensus.py`
- Configurable threshold (CONSENSUS_THRESHOLD environment variable)
- Time-based confidence weighting
- Cooldown periods to prevent rapid re-triggering

## Hardware Abstraction

### AI Accelerator Support
- Auto-detection via `FRIGATE_DETECTOR=auto`
- Platform-specific device mappings in docker-compose.yml
- Performance targets: Coral (15-20ms), Hailo (10-25ms), GPU (8-12ms)
- Coral TPU requires Python 3.8 runtime environment

### GPIO Simulation
- Automatically enabled on non-Raspberry Pi systems
- Override with `GPIO_SIMULATION=true/false`
- All pin assignments configurable via environment variables

## Security Architecture

### Certificate Management
- Production requires running `./scripts/generate_certs.sh custom`
- Default certificates are intentionally insecure for development
- TLS enabled via `MQTT_TLS=true` environment variable

### Camera Credentials
- Supports multiple credential sets: `CAMERA_CREDENTIALS=username:password,username2:password2`
- Automatic credential testing during discovery
- MAC address tracking prevents IP-based spoofing
- **NEVER hardcode credentials in code files** - Always use environment variables
- Pass credentials via environment when running tests: `CAMERA_CREDENTIALS=admin:password pytest tests/`

## Testing Strategy

### Test Categories
- **Unit Tests**: `test_consensus.py`, `test_detect.py`, `test_trigger.py`
- **Integration Tests**: `test_integration_e2e.py`, `test_hardware_integration.py`
- **Development Testing**: Use `GPIO_SIMULATION=true`, mock MQTT broker available

### Test Fixing Guidelines
1. **Test the actual code, not a mock** - Only mock external dependencies
2. **Fix the code, not just the test** - If test reveals bug, fix source code
3. **Preserve test intent** - Don't remove assertions to make tests pass
4. **Minimal mocking** - Only mock external I/O (files, network, hardware)
5. **Test real behavior** - Include edge cases and error conditions

### Integration Testing Philosophy
**Avoid mocking functions and files within wildfire-watch**:

1. **Never mock internal modules** - Don't mock `consensus`, `trigger`, `detect`, etc.
   - ❌ Bad: `@patch('consensus.FireConsensus')`
   - ✅ Good: Actually instantiate and use `FireConsensus` class

2. **Only mock external dependencies**:
   - ✅ Mock: `RPi.GPIO`, `docker`, `requests`
   - ✅ Mock: File I/O, network calls, hardware interfaces
   - ✅ Mock: Time delays (`time.sleep`) for faster tests
   - ❌ **DO NOT Mock: `paho.mqtt.client`** - Use real MQTT broker

3. **MQTT Integration Testing Requirements**:
   - **Always use real MQTT broker** - Use `TestMQTTBroker` class
   - Test actual MQTT message flow between components
   - Verify real connection handling and reconnection logic
   - **Never mock MQTT client** - This prevents testing the actual communication layer

### Container Management and Test Isolation Best Practices

#### Use DockerContainerManager for Container Tests
When writing tests that use Docker containers, always use `DockerContainerManager` for proper isolation and cleanup:

```python
# ✅ Good: Proper container management with isolation
@pytest.fixture
def mqtt_broker(self, docker_container_manager):
    container = docker_container_manager.start_container(
        image="eclipse-mosquitto:2.0",
        name=docker_container_manager.get_container_name("mqtt"),
        config={'ports': {'1883/tcp': None}}  # Dynamic port allocation
    )
    return container

# ❌ Bad: Manual container management without isolation
def test_something():
    client = docker.from_env()
    container = client.containers.run("image", name="test", detach=True)
    # No cleanup, no isolation, conflicts with parallel tests
```

#### Use ParallelTestContext for Multi-Service Tests
For complex tests involving multiple services, use `ParallelTestContext` to get proper topic namespacing and environment variables:

```python
# ✅ Good: Parallel test context with automatic topic prefixing
@pytest.fixture(autouse=True)
def setup_parallel_context(self, parallel_test_context, test_mqtt_broker, docker_container_manager):
    self.parallel_context = parallel_test_context
    self.mqtt_broker = test_mqtt_broker
    self.docker_manager = docker_container_manager
    
    # Services get proper environment with topic prefixes
    env_vars = self.parallel_context.get_service_env('fire_consensus')
    # env_vars includes MQTT_TOPIC_PREFIX=test/worker_id automatically

# ❌ Bad: Manual environment setup without test isolation
def test_services():
    env = {'MQTT_BROKER': 'localhost', 'MQTT_PORT': '1883'}
    # No topic prefixing = tests conflict with each other
```

#### Topic Namespace Strategy for Test Isolation
All MQTT topics must be namespaced to prevent test interference:

```python
# ✅ Good: Proper topic namespacing
def test_fire_detection(self, parallel_test_context):
    topic_prefix = parallel_test_context.get_topic_prefix()  # Returns "test/worker_id"
    
    # Publish to namespaced topic
    mqtt_client.publish(f"{topic_prefix}/fire/detection", payload)
    
    # Subscribe to namespaced topics
    mqtt_client.subscribe(f"{topic_prefix}/fire/trigger")
    
    # Services automatically use prefixed topics via MQTT_TOPIC_PREFIX env var

# ❌ Bad: No topic namespacing
def test_fire_detection():
    mqtt_client.publish("fire/detection", payload)  # Conflicts with other tests
    mqtt_client.subscribe("fire/trigger")  # Gets messages from other tests
```

#### Container Environment Best Practices
Always set proper environment variables for container isolation:

```python
# ✅ Good: Complete environment setup with topic prefixing
def start_consensus_service(self, docker_container_manager):
    worker_id = docker_container_manager.worker_id
    topic_prefix = f"test/{worker_id}"
    
    config = {
        'environment': {
            'MQTT_BROKER': 'localhost',
            'MQTT_PORT': str(self.mqtt_broker.port),
            'MQTT_TOPIC_PREFIX': topic_prefix,  # Critical for test isolation
            'CONSENSUS_THRESHOLD': '1',  # Test-specific config
            'LOG_LEVEL': 'DEBUG'
        },
        'network_mode': 'host'  # Required for localhost MQTT access
    }
    
    return docker_container_manager.start_container("service:latest", name, config)

# ❌ Bad: Missing topic prefix and test-specific configuration
def start_service():
    config = {
        'environment': {
            'MQTT_BROKER': 'localhost',
            'MQTT_PORT': '1883'
            # Missing MQTT_TOPIC_PREFIX = topic conflicts
            # Missing test-specific timeouts = tests hang
        }
    }
```

#### Test Data Isolation Patterns
Ensure test data doesn't interfere between parallel workers:

```python
# ✅ Good: Worker-specific temporary directories
@pytest.fixture
def temp_config_dir(self, docker_container_manager):
    temp_dir = tempfile.mkdtemp(prefix=f"test_{docker_container_manager.worker_id}_")
    yield temp_dir
    shutil.rmtree(temp_dir, ignore_errors=True)

# ✅ Good: Worker-specific container names
container_name = docker_container_manager.get_container_name('frigate')
# Returns: wf-{worker_id}-frigate

# ❌ Bad: Hardcoded names and paths cause conflicts
temp_dir = "/tmp/test_config"  # Multiple workers conflict
container_name = "test_frigate"  # Name collision between workers
```

#### Error Handling and Cleanup
Always implement proper cleanup even when tests fail:

```python
# ✅ Good: Comprehensive cleanup with try/finally
def test_complex_pipeline(self, docker_container_manager):
    containers = {}
    try:
        containers['service1'] = docker_container_manager.start_container(...)
        containers['service2'] = docker_container_manager.start_container(...)
        
        # Test logic here
        
    finally:
        # DockerContainerManager handles container cleanup automatically
        # But clean up other resources manually
        for resource in test_resources:
            try:
                resource.cleanup()
            except:
                pass  # Don't fail cleanup due to cleanup errors
```

#### When to Use Each Testing Approach

**Use `DockerContainerManager` when:**
- Testing individual services with containers
- Need proper container isolation and cleanup
- Want dynamic port allocation to avoid conflicts

**Use `ParallelTestContext` when:**
- Testing multiple interacting services
- Need complete environment variable setup
- Want automatic MQTT topic namespacing

**Use both together when:**
- Running complex E2E tests
- Testing complete system integration
- Need maximum test isolation and reliability

## Common Environment Variables

### Core Settings
- `CONSENSUS_THRESHOLD=2` - Cameras required for fire trigger
- `MIN_CONFIDENCE=0.7` - AI detection confidence threshold
- `MAX_ENGINE_RUNTIME=1800` - Safety limit in seconds
- `FRIGATE_DETECTOR=auto` - AI accelerator selection

### Development Settings
- `LOG_LEVEL=DEBUG` - Verbose logging
- `GPIO_SIMULATION=true` - Safe testing without hardware
- `DISCOVERY_INTERVAL=300` - Camera discovery frequency

## Deployment Considerations

### Balena Cloud
- Fleet management for multiple edge devices
- Environment variables managed via Balena dashboard
- Automatic updates and rollback support

### Resource Requirements
- Minimum 4GB RAM for Frigate + AI detection
- 32GB+ storage for video retention
- USB 3.0 for Coral TPU, PCIe for Hailo
- GPIO access required for pump control

### Network Architecture
- Cameras discovered via ONVIF, mDNS, port scanning
- MQTT broker creates isolated network (192.168.100.0/24)
- Frigate UI accessible on port 5000
- TLS encryption for production MQTT (port 8883)

## File Organization Guidelines

### Directory Structure
- **tmp/** - Temporary test scripts, debugging files, intermediate outputs
- **output/** - Final test results, reports, converted models
- **scripts/** - Permanent utility scripts (kept in repository)
- **docs/** - Documentation files and guides
- **converted_models/** - Model conversion scripts and utilities
- **tests/** - Test files (must start with `test_` prefix)

### Examples:
```bash
# Temporary files
tmp/test_tensorrt_fix.py
tmp/debug_yolov9.py

# Output files
output/test_results.log
output/model_conversion_report.md

# Permanent scripts
scripts/validate_models.py
scripts/generate_certs.sh
```

## Development Workflow for Non-Trivial Work

For any work >30 minutes or involving multiple files:

1. **Create a Plan File**
   - Name: `[feature_name]_plan.md` in appropriate directory
   - Include: Overview, phases, timeline, technical requirements

2. **Execute Plan with Progress Updates**
   - Mark phases: `## Phase X: [Name] - ⏳ IN PROGRESS` → `✅ COMPLETE`
   - Document deviations or issues encountered

3. **Testing Requirements**
   - Run all tests related to changed code
   - Fix the program's code first, not the test
   - Document skipped tests with specific reasons

### Plan File Template
```markdown
# [Feature Name] Implementation Plan

## Overview
Brief description of what will be accomplished

## Phases
### Phase 1: [Name] - ⏳ PENDING
- Task 1
- Task 2

## Testing
- List of test files that will be affected
- Expected test changes

## Progress Notes
[Add progress updates here as work proceeds]

## Test Results
- Tests run: X
- Tests passed: Y
- Tests failed: Z
- Tests skipped: N (with reasons)
```

## Documentation Best Practices

### Sphinx-Compatible Documentation
All Python code should follow Sphinx documentation standards. Use Google-style docstrings.

#### Documentation Principles
1. **Component Connectivity**: Document MQTT topic connections
2. **Parameter Implications**: Document non-obvious effects
3. **Side Effects**: Document MQTT publishes
4. **Error Handling**: Document exceptions
5. **Thread Safety**: Document synchronization requirements
6. **MQTT Topics**: Document all pub/sub topics

#### Example Docstring
```python
def process_detection(self, detection: Dict[str, Any]) -> bool:
    """Process a fire detection from a camera and update consensus state.
    
    Args:
        detection: Detection data containing:
            - camera_id (str): Unique camera identifier
            - confidence (float): Detection confidence (0.0-1.0)
            - object_type (str): 'fire' or 'smoke'
    
    Returns:
        bool: True if consensus threshold is met
        
    Side Effects:
        - Updates internal detection history
        - May publish to 'trigger/fire_detected' if consensus reached
        
    MQTT Topics:
        - Subscribes to: frigate/+/fire, frigate/+/smoke
        - Publishes to: trigger/fire_detected (on consensus)
        
    Thread Safety:
        This method is thread-safe due to internal locking
    """
```

## AI Assistant Guidelines

### MCP Tool Usage for Enhanced Development

#### Critical Safety and Security Analysis
For a safety-critical fire suppression system, use specialized tools:

- **`mcp__zen__secaudit`** - Comprehensive security audit for OWASP analysis, vulnerability assessment
  - Use for GPIO control code, MQTT authentication, certificate management
  - Choose `pro` model for deep security analysis
  - Focus on hardware control safety and network security

- **`mcp__zen__codereview`** - Professional code review for bugs, security, performance
  - Essential for pump control logic, consensus algorithms, camera detection
  - Use for safety-critical paths and hardware interfaces
  - Identifies subtle issues that could cause system failures

#### Development Workflow Tools
Follow the recommended workflow: **Design → Review → Implement → Test → Precommit**

- **`mcp__zen__planner`** - Interactive step-by-step planning for complex features
  - Use for new AI model integrations, multi-service features
  - Break down complex tasks like Hailo integration or Coral TPU support

- **`mcp__zen__debug`** - Systematic investigation and root cause analysis
  - Use when encountering specific errors or mysterious behaviors
  - Provides step-by-step investigation workflow

- **`mcp__zen__analyze`** - Architecture and code pattern analysis
  - Use to understand microservices communication patterns
  - Analyze MQTT message flows and service dependencies

- **`mcp__zen__testgen`** - Comprehensive test generation with edge cases
  - Create test suites for hardware interfaces, consensus logic
  - Generate tests for edge cases in fire detection algorithms

- **`mcp__zen__precommit`** - Pre-commit validation of git changes
  - Validate changes before commits, especially for safety-critical code
  - Ensure test coverage and documentation updates

#### Collaborative Analysis and Decision Making

- **`mcp__zen__challenge`** - Critical challenge prompt to question assumptions
  - Use before implementing safety-critical features
  - Challenge design decisions for pump control, fire detection thresholds
  - Prevents automatic agreement, encourages thorough analysis

- **`mcp__zen__consensus`** - Multi-model perspective gathering
  - Get diverse expert opinions on architectural decisions
  - Use for complex design choices like consensus algorithms
  - Combine different AI models for comprehensive analysis

- **`mcp__zen__chat`** - Collaborative thinking and brainstorming
  - Bounce ideas for new features, optimization strategies
  - Get second opinions on technical approaches
  - Use for general development discussions

#### Model Selection Strategy
Choose models based on task complexity and domain:

- **Gemini Pro**: Large context analysis, architecture decisions, security reviews
- **o3**: Logic problems, algorithmic optimization, mathematical reasoning
- **Enable web search**: For current API documentation, best practices, error solutions

#### Plan Review Requirements
**All implementation plans MUST be reviewed by advanced models before finalization:**

- **Use o3 or Gemini Pro** to review plans created by other models
- **Critical for safety systems**: Fire suppression, GPIO control, consensus algorithms
- **Review focus areas**:
  - Technical feasibility and approach
  - Safety considerations and edge cases
  - Resource requirements and timelines
  - Integration points and dependencies
  - Testing and validation strategies
- **Implementation**: Use `mcp__zen__consensus` with o3 and Gemini as reviewers

#### Tool Selection Decision Flow
1. **Specific error or bug?** → `mcp__zen__debug`
2. **Want to find code issues?** → `mcp__zen__codereview`
3. **Need security analysis?** → `mcp__zen__secaudit`
4. **Understand code architecture?** → `mcp__zen__analyze`
5. **Need comprehensive tests?** → `mcp__zen__testgen`
6. **Complex feature planning?** → `mcp__zen__planner`
7. **Want to challenge assumptions?** → `mcp__zen__challenge`
8. **Need multiple perspectives?** → `mcp__zen__consensus`
9. **Ready to commit changes?** → `mcp__zen__precommit`

### Traditional Model Usage Guidelines

#### Debugging and Code Analysis
**Use Gemini for:**
- Large context analysis (>10 files or >5000 lines)
- Complex debugging across multiple files
- Architecture analysis
- Performance analysis across components

**Use o3 for:**
- Tricky logic problems
- Algorithm optimization
- Single function debugging
- Mathematical reasoning
- Edge case identification

### Information Accuracy Guidelines
**Always use web search for:**
- **All uncertain APIs** - Never guess API parameters, methods, or usage patterns
- Library features and compatibility
- Error message solutions
- Breaking changes between versions

**For difficult debugging problems:**
- **Search local .md files first** - Look for similar problems and solutions in project documentation
- Use `Glob` tool to find relevant docs: `*.md`, `docs/*.md`, `*SUMMARY.md`
- Check `*_PLAN.md`, `*_SUMMARY.md`, `*_GUIDE.md` files for past solutions

**Never speculate** - Search and verify all technical details.