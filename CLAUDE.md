# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Documentation Status

This documentation reflects the **current state** of the codebase to ensure accuracy for AI assistants and developers. Patterns are marked with status indicators:

- **[STABLE & IN USE]** - Current production patterns, use these
- **[TARGET PATTERN - IN DEVELOPMENT]** - Future patterns being developed, do not use yet
- **[DEPRECATED - REQUIRED FOR LEGACY]** - Old patterns still needed, continue using until migration

When in doubt, use the patterns marked as [STABLE & IN USE].

## Project Overview

Wildfire Watch is an automated fire detection and suppression system that runs on edge devices. It uses AI-powered cameras with multi-camera consensus to detect fires and automatically activates sprinkler systems via GPIO control.

## Tool Usage Guidelines

### Parallel Tool Execution
When performing multiple independent operations, use parallel tool calls in a single message for optimal performance:

**Benefits:** Faster execution, more efficient workflow, better performance for complex tasks

## Development Commands

### Testing
```bash
# AUTOMATIC PYTHON VERSION SELECTION (Recommended)
# Run all tests with automatic Python version selection
./scripts/run_tests_by_python_version.sh --all

# Run specific Python version tests
./scripts/run_tests_by_python_version.sh --python312  # Most tests
./scripts/run_tests_by_python_version.sh --python310  # YOLO-NAS/super-gradients  
./scripts/run_tests_by_python_version.sh --python38   # Coral TPU/TensorFlow Lite

# Run specific test with auto-detection
./scripts/run_tests_by_python_version.sh --test tests/test_detect.py

# Validate Python environment
./scripts/run_tests_by_python_version.sh --validate

# MANUAL PYTHON VERSION SELECTION (Advanced)
python3.12 -m pytest -c pytest-python312.ini
python3.10 -m pytest -c pytest-python310.ini
python3.8 -m pytest -c pytest-python38.ini

# SPECIFIC TEST CATEGORIES
python3.12 -m pytest tests/ -v -m "not slow and not infrastructure_dependent"
python3.10 -m pytest tests/ -v -m "yolo_nas"
python3.8 -m pytest tests/ -v -m "coral_tpu"

# Disable timeouts for debugging
python3.12 -m pytest tests/ -v --timeout=0
```

### Test Timeout Configuration
- **pytest-python312.ini**: 1 hour per test, 2 hour session timeout  
- **pytest-python310.ini**: 2 hours per test, 4 hour session timeout (for training tests)
- **pytest-python38.ini**: 2 hours per test, 4 hour session timeout (for model conversion)

These timeouts ensure infrastructure-heavy tests complete successfully and prevent indefinite hangs.

### Python Version
This project requires Python 3.12. All commands should use `python3.12` and `pip3.12`.

**Exceptions**:

1. **Coral TPU**: Requires Python 3.8 for `tflite_runtime` compatibility
   - Use `python3.8` instead of `python3.12`
   - Install: `python3.8 -m pip install tflite-runtime`
   - Run `./scripts/check_coral_python.py` to verify setup

2. **YOLO-NAS Training**: Requires Python 3.10 for `super-gradients` compatibility
   - Use `python3.10` instead of `python3.12`
   - Install: `python3.10 -m pip install super-gradients`
   - Training scripts: `python3.10 converted_models/train_yolo_nas.py`

## Architecture Overview

### Microservices Communication
All services communicate via MQTT broker with the following data flow:

1. **Camera Detector** → Discovers IP cameras, publishes to `cameras/discovered`
2. **Security NVR (Frigate)** → AI detection, publishes to `frigate/*/fire` and `frigate/*/smoke`
3. **Fire Consensus** → Validates detections, publishes to `trigger/fire_detected`
4. **GPIO Trigger** → Controls pump hardware, publishes to `gpio/status`
5. **Telemetry** → Health monitoring, publishes to `telemetry/*`

### Service Dependencies
```
mqtt_broker (core)
├── camera_detector (needs MQTT)
├── security_nvr (needs MQTT + camera config)
├── fire_consensus (needs MQTT + camera data)
├── gpio_trigger (needs MQTT + consensus)
└── cam_telemetry (needs MQTT)
```

### Key Configuration Files
- `docker-compose.yml` - Service orchestration with healthchecks and dependencies
- `.env` - Environment variables for all services
- `certs/` - TLS certificates (generated by `./scripts/generate_certs.sh`)
- `frigate_config/config.yml` - Dynamically generated by camera_detector

## Development Patterns

### Base Class Architecture
**Recommended base classes for services (78% code reduction achieved):**

1. **MQTTService** (`utils/mqtt_service.py`)
   - Handles all MQTT connectivity with automatic reconnection
   - Provides thread-safe publishing with offline message queuing
   - Manages Last Will Testament (LWT) for service health
   - **Recommended** for all services that use MQTT communication

2. **ThreadSafeService** (`utils/thread_manager.py`)
   - Use `SafeTimerManager` for all timer operations
   - Use `BackgroundTaskRunner` for periodic tasks
   - Provides graceful shutdown handling
   - **Recommended** instead of creating raw threads

3. **ConfigBase** (`utils/config_base.py`)
   - Schema-based configuration with validation
   - Type conversion and cross-service validation
   - **Recommended** for configuration management

4. **HealthReporter** (`utils/health_reporter.py`)
   - Standardized health monitoring and reporting
   - Automatic system metrics collection
   - **Recommended** for consistent health reporting

**Current Implementation Status:**
- ✅ **Fire Consensus**: Uses MQTTService, ThreadSafeService, ConfigBase
- ✅ **Camera Detector**: Uses MQTTService, ThreadSafeService, ConfigBase
- ⚡ **GPIO Trigger**: Uses ConfigBase only (intentional design for hardware reliability)

**Example Implementation:**
```python
class MyService(MQTTService, ThreadSafeService):
    def __init__(self):
        config = MyServiceConfig()  # Inherits from ConfigBase
        MQTTService.__init__(self, "my_service", config.__dict__)
        ThreadSafeService.__init__(self)
        self.health_reporter = MyHealthReporter(self)  # Inherits from HealthReporter
```

**Exception: GPIO PumpController**
The PumpController intentionally does NOT inherit from MQTTService for safety-critical reasons:
- Direct hardware control requires minimal dependencies
- Network failures must not prevent pump activation
- Lower latency for emergency response
- Simpler code path reduces failure modes

### Service Initialization Pattern
**Recommended initialization order for services using MQTTService:**

1. **Create all components first** (health reporter, task runners, etc.)
2. **Initialize MQTT connection** after components are ready
3. **Wait for connection verification** before starting operations (for MQTTService-based services)
4. **Start health reporting** only after connection is verified

**Implementation Notes:**
- Check `main()` for redundant initialization calls
- Update callback signatures for paho-mqtt compatibility: `def _on_connect(self, client, userdata, flags, rc, properties=None)`
- Services using MQTTService should call `wait_for_connection(timeout=30)`

**Example for MQTTService-based services:**
```python
def __init__(self):
    # 1. Initialize components
    self.health_reporter = MyHealthReporter(self)
    self._start_monitoring_tasks()
    
    # 2. Connect to MQTT
    self.connect()
    
    # 3. Wait and verify
    if self.wait_for_connection(timeout=30):
        self.health_reporter.start_health_reporting()
    else:
        raise RuntimeError("Failed to connect to MQTT broker")
```

**Note:** PumpController connects automatically in `__init__` and does not have `wait_for_connection()` method

### Adding New Camera Support
1. Modify `camera_detector/detect.py` discovery methods
2. Update camera credential handling in environment variables
3. Test RTSP stream validation
4. Ensure MAC address tracking works for persistent identification

### AI Model Integration
- Models stored in `converted_models/` with conversion script
- Supports Coral TPU (.tflite), Hailo (.hef), ONNX, TensorRT formats
- Update `security_nvr/nvr_base_config.yml` for new models
- **Model sizes**: 640x640 (optimal accuracy), 416x416 (balanced), 320x320 (edge devices)
- **Default**: 640x640 for optimal fire detection accuracy
- **Coral TPU requires Python 3.8** for tflite_runtime compatibility
- **Always use QAT (Quantization-Aware Training)** for INT8 formats when available
- Model accuracy validation ensures <2% degradation for production
- **Calibration data**: https://huggingface.co/datasets/mailseth/wildfire-watch/resolve/main/wildfire_calibration_data.tar.gz?download=true

#### Frigate EdgeTPU Model Requirements
- **Input Tensor Type**: Frigate's EdgeTPU detector sends UINT8 data to models
- **Model Compatibility**: Ensure TFLite models accept UINT8 input tensors (not INT8 or FLOAT32)
- **Conversion Settings**: Use `converter.inference_input_type = tf.uint8` when creating TFLite models
- **Verification**: Test models with pycoral before deployment to confirm UINT8 input compatibility

#### Model Conversion Timeouts
- **ONNX**: ~2-5 minutes
- **TFLite**: ~15-30 minutes (INT8 quantization)
- **TensorRT**: ~30-60 minutes (engine optimization)
- **OpenVINO**: ~10-30 minutes
- **Hailo**: ~20-40 minutes (Docker compilation)

If conversions timeout:
1. Use smaller model sizes (320x320 instead of 640x640)
2. Reduce calibration dataset size
3. Run on more powerful hardware
4. Use pre-converted models when available

#### Model Conversion Optimization
**Strongly recommended: implement caching to avoid repeated work:**

1. **Caching System** (10-50x speedup achieved)
   - Cache converted models with unique keys based on model/size/format/precision
   - Check cache before any conversion operation
   - Store cache metadata for validation

2. **Parallel Conversion**
   - Use `ThreadPoolExecutor` for converting multiple formats simultaneously
   - Convert to ONNX first, then use it as source for other formats
   - Maximum 4 parallel workers to avoid resource exhaustion

3. **Tiered Testing Approach**
   - **Unit tests**: Mock conversions entirely
   - **Integration tests**: Use cached models
   - **E2E tests**: Real conversions (run nightly only)

**Example Cache Implementation:**
```python
cache_key = f"{model_name}_{size}x{size}_{format}_{precision}"
cached_path = cache_manager.get_cached_model(cache_key)
if cached_path:
    return cached_path
# Perform conversion only if not cached
result = convert_model(...)
cache_manager.cache_model(cache_key, result)
```

### GPIO Safety Systems
- All pump control in `gpio_trigger/trigger.py` uses state machine pattern
- Maximum runtime protection (default 30 minutes)
- Automatic refill calculation based on runtime
- Hardware simulation mode for development on non-Pi systems

### Multi-Camera Consensus Logic
- Located in `fire_consensus/consensus.py`
- Configurable threshold (CONSENSUS_THRESHOLD environment variable)
- Time-based confidence weighting
- Cooldown periods to prevent rapid re-triggering

## Hardware Abstraction

### AI Accelerator Support
- Auto-detection via `FRIGATE_DETECTOR=auto`
- Platform-specific device mappings in docker-compose.yml
- Performance targets: Coral (15-20ms), Hailo (10-25ms), GPU (8-12ms)
- Coral TPU requires Python 3.8 runtime environment

### GPIO Simulation
- Automatically enabled on non-Raspberry Pi systems
- Override with `GPIO_SIMULATION=true/false`
- All pin assignments configurable via environment variables

## Security Architecture

### Certificate Management
- Production requires running `./scripts/generate_certs.sh custom`
- Default certificates are intentionally insecure for development
- TLS enabled via `MQTT_TLS=true` environment variable

### Camera Credentials
- Supports multiple credential sets: `CAMERA_CREDENTIALS=username:password,username2:password2`
- Automatic credential testing during discovery
- MAC address tracking prevents IP-based spoofing
- **NEVER hardcode credentials in code files** - Always use environment variables
- Pass credentials via environment when running tests: `CAMERA_CREDENTIALS=admin:password pytest tests/`

## Testing Strategy

### Test Categories
- **Unit Tests**: `test_consensus.py`, `test_detect.py`, `test_trigger.py`
- **Integration Tests**: `test_integration_e2e.py`, `test_hardware_integration.py`
- **Development Testing**: Use `GPIO_SIMULATION=true`, mock MQTT broker available

### Test Fixing Guidelines
1. **Test the actual code, not a mock** - Only mock external dependencies
2. **Fix the code, not just the test** - If test reveals bug, fix source code
3. **Preserve test intent** - Don't remove assertions to make tests pass
4. **Minimal mocking** - Only mock external I/O (files, network, hardware)
5. **Test real behavior** - Include edge cases and error conditions
6. **Fix import paths** - Use `test_utils` not `utils` for test helpers
7. **Use pump_controller_factory** - Factory fixture handles all configuration
8. **Use correct config keys** - IGNITION_START_DURATION not ENGINE_START_DURATION

### Integration Testing Best Practices
**Recommended patterns for reliable integration tests:**

1. **NEVER mock internal modules** - Test failures will result from mocking:
   - ❌ **FORBIDDEN**: `@patch('fire_consensus.consensus.FireConsensus')`
   - ❌ **FORBIDDEN**: `@patch('paho.mqtt.client.Client')`
   - ✅ **REQUIRED**: Use actual service instances with `TestMQTTBroker`

2. **ALWAYS use real MQTT communication**:
   - **REQUIRED**: Use `TestMQTTBroker` from `test_utils.mqtt_test_broker`
   - **REQUIRED**: Test actual message flow between services
   - **REQUIRED**: Verify connection handling and reconnection
   - **FORBIDDEN**: Mock MQTT client or broker
   - **Note**: The test broker uses dynamic port allocation to avoid conflicts

3. **MUST implement topic namespacing**:
   - **REQUIRED**: Prefix all topics with `worker_id` for parallel test isolation
   - **REQUIRED**: Use `mqtt_topic_factory` fixture from conftest.py
   - **Example**: `topic = mqtt_topic_factory("fire/trigger")`

4. **Event-driven synchronization ONLY**:
   - ❌ **FORBIDDEN**: `time.sleep(5)` - causes flaky tests
   - ✅ **REQUIRED**: Use `Event()` with callbacks for synchronization
   - **Example**:
     ```python
     trigger_event = Event()
     client.message_callback_add("*/fire/trigger", 
                                lambda c,u,m: trigger_event.set())
     assert trigger_event.wait(timeout=10), "Fire not triggered"
     ```

5. **Test both success AND failure scenarios**:
   - **REQUIRED**: Test error conditions, not just happy paths
   - **REQUIRED**: Test service disconnections and recovery
   - **REQUIRED**: Test invalid inputs and edge cases

### MQTT Configuration for Parallel Testing
**Best practices to prevent ConnectionRefusedError in parallel test execution:**

#### The Problem
Services like GPIO trigger load configuration at module import time, before test fixtures can set worker-specific MQTT broker details. This causes connection failures in parallel tests as all workers try to connect to the same (non-existent) broker.

#### The Solution: Dependency Injection Pattern

1. **For GPIO Trigger Service**:
   
   **[MIGRATION COMPLETED ✅] - Current Pattern:**
   ```python
   # Use the pump_controller_factory fixture
   controller = pump_controller_factory(
       mqtt_broker=conn_params['host'],
       mqtt_port=conn_params['port'],
       topic_prefix=topic_prefix,
       priming_duration=0.2,
       # other config overrides
   )
   controller.connect()  # Connect when ready
   ```
   
   **Migration Status**: ✅ COMPLETED
   - Legacy CONFIG dictionary has been removed
   - All tests now use pump_controller_factory fixture
   - PumpControllerConfig provides type-safe configuration
   - See docs/GPIO_CONFIG_MIGRATION.md for details

2. **For Services Using ConfigBase (fire_consensus, camera_detector)**:
   ```python
   # Set environment variables BEFORE creating service
   monkeypatch.setenv("MQTT_BROKER", conn_params['host'])
   monkeypatch.setenv("MQTT_PORT", str(conn_params['port']))
   monkeypatch.setenv("TOPIC_PREFIX", topic_prefix)
   
   # Service reads configuration at runtime
   service = FireConsensus()  # ConfigBase loads from environment
   ```

3. **Key Implementation Details**:
   - **PumpController accepts config parameter**: `PumpController(config=PumpControllerConfig())`
   - **PumpControllerConfig uses ConfigBase**: Runtime loading from environment
   - **Global CONFIG dictionary**: ❌ REMOVED - no longer used
   - **Test fixture**: `pump_controller_factory` - creates configured instances
   - **Helper function**: `create_pump_controller_with_config()` - available for complex cases

#### Best Practices Summary

1. **Never rely on import-time configuration loading**
2. **Always use dependency injection for configuration**
3. **Set environment variables BEFORE service instantiation**
4. **Use worker-specific MQTT broker instances**
5. **Implement proper cleanup in fixtures**

### Common Test Patterns and Fixes

#### Configuration in GPIO Tests
GPIO tests now use the `pump_controller_factory` fixture which handles all configuration:

```python
# Create controller with test configuration
controller = pump_controller_factory(
    mqtt_broker=conn_params['host'],
    mqtt_port=conn_params['port'],
    topic_prefix=topic_prefix,
    priming_duration=0.2,
    max_engine_runtime=60
)
controller.connect()  # If auto_connect=False was specified
```

#### Test Import Paths
Test utilities have been moved to `tests/test_utils/`. Update imports accordingly:

```python
# WRONG - Old import paths
from tests.mqtt_test_broker import MQTTTestBroker
from utils.helpers import DockerContainerManager

# CORRECT - New import paths
from tests.test_utils.mqtt_test_broker import MQTTTestBroker
from tests.test_utils.helpers import DockerContainerManager
```

#### Hardware Test Python Version Requirements
Different hardware requires specific Python versions:

- **Coral TPU**: Python 3.8 only (tflite_runtime compatibility)
- **YOLO-NAS/Hailo**: Python 3.10 (super-gradients compatibility)
- **TensorRT/GPU**: Python 3.12 compatible

#### Timing Configuration Validation
For pump safety tests, ensure timing parameters are self-consistent:

```python
# Set reduced timing for tests
os.environ['MAX_ENGINE_RUNTIME'] = '15'     # Total runtime
os.environ['PRIMING_DURATION'] = '2'        # Startup priming
os.environ['IGNITION_START_DURATION'] = '3' # Engine start (not ENGINE_START_DURATION)
os.environ['RPM_REDUCTION_LEAD'] = '5'      # Time before shutdown

# RPM reduction occurs at: MAX_ENGINE_RUNTIME - RPM_REDUCTION_LEAD
# In this example: 15 - 5 = 10 seconds
```

6. **Service Configuration Pattern**:
   - **Current Implementation**: Mixed patterns exist
   - **FireConsensus/CameraDetector**: Read from environment via ConfigBase
   - **PumpController**: Supports both patterns:
     ```python
     # Method 1: Environment variables (auto-config)
     controller = PumpController()
     
     # Method 2: Dependency injection (for tests)
     config = PumpControllerConfig()
     controller = PumpController(config=config)
     ```

7. **GPIO PumpController Implementation Details**:
   - **Architecture**: PumpController does NOT inherit from MQTTService base class (intentional for reliability)
   - **Connection**: PumpController connects to MQTT automatically in `__init__`
   - **No wait_for_connection**: This method doesn't exist on PumpController
   - **Testing Pattern**:
     ```python
     # Create controller - it connects automatically
     controller = PumpController()
     
     # Optional: Add small delay to ensure connection is established
     time.sleep(1.0)
     
     # Controller is ready to use
     ```
   - **Configuration**: Uses PumpControllerConfig with dependency injection

### GPIO Testing Best Practices
**Based on extensive testing experience (40+ test scenarios), follow these practices:**

1. **NO mocking of GPIO internals**:
   - ✅ **REQUIRED**: Use real PumpController instances
   - ✅ **REQUIRED**: Use real GPIO module or built-in simulation
   - ❌ **FORBIDDEN**: Mock GPIO operations or state machine
   - **Rationale**: Tests must verify actual hardware behavior

2. **Use existing GPIO test utilities**:
   ```python
   from utils.gpio_test_helpers import (
       wait_for_state,           # Wait for specific pump states
       verify_pin_states,        # Verify multiple pins at once
       PinMonitor,              # Monitor pin changes over time
       verify_safe_shutdown_state  # Ensure safe pin states
   )
   ```

3. **Proper GPIO cleanup between tests**:
   - **REQUIRED**: Reset all pins to LOW state
   - **REQUIRED**: Clear GPIO._state dictionary
   - **REQUIRED**: Cancel any active timers
   - **Example**: Use gpio_test_setup fixture which handles cleanup

4. **Test with missing sensors**:
   - **REQUIRED**: Test operation without any sensors
   - **REQUIRED**: Test with various sensor combinations
   - **Example**: Set sensor pins to 0 to disable

5. **Error state is last resort**:
   - **Only enter ERROR for**: Main valve failure, dry run >5min, engine start failure
   - **Automatic recovery for**: GPIO retries, MQTT disconnection, non-critical failures

### Docker Container Management
**Use DockerContainerManager for reliable container testing:**

1. **Container Naming Requirements**:
   - **REQUIRED**: Worker-specific names: `f"wf-{worker_id}-{service}"`
   - **FORBIDDEN**: Hardcoded container names in parallel tests
   - **Example**: `manager.get_container_name("mqtt-broker")`

2. **Container Lifecycle Management**:
   - **REQUIRED**: Use atomic operations with thread-safe locks
   - **REQUIRED**: Implement retry logic for Docker API errors
   - **REQUIRED**: Cleanup sequence: Stop → Wait → Remove with verification
   - **REQUIRED**: Define health checks for ALL containers

3. **Health Check Implementation**:
   ```python
   def mqtt_health_check(container):
       try:
           exit_code, _ = container.exec_run(
               "mosquitto_sub -h localhost -p 1883 -t test -C 1 -W 1"
           )
           return exit_code == 0
       except:
           return False
   
   container = manager.start_container(
       image="eclipse-mosquitto",
       health_check_fn=mqtt_health_check
   )
   ```

4. **Cleanup Requirements**:
   - **REQUIRED**: Cleanup old containers before creating new ones
   - **REQUIRED**: Use `force=True` for cleanup in fixtures
   - **REQUIRED**: Register cleanup handlers for unexpected exits

#### Multi-Service Test Patterns

**[STABLE & IN USE] - Both patterns are actively used:**

1. **ParallelTestContext** - For complex integration tests:
   ```python
   from test_utils.helpers import ParallelTestContext
   env_vars = parallel_context.get_service_env('fire_consensus')
   ```

2. **mqtt_topic_factory** - For topic namespacing in all tests:
   ```python
   full_topic = mqtt_topic_factory("fire/trigger")
   topic_prefix = full_topic.rsplit('/', 1)[0]
   ```

#### Error Handling Patterns
- **Retry transient errors**: "removal already in progress", "network has active endpoints"
- **Wait for operations**: Allow time between stop and remove
- **Disconnect before network removal**: Prevent "active endpoints" errors

#### When to Use Each Testing Approach

**Use `DockerContainerManager` when:**
- Testing individual services with containers
- Need proper container isolation and cleanup
- Want dynamic port allocation to avoid conflicts

**Use `ParallelTestContext` when:**
- Testing multiple interacting services
- Need complete environment variable setup for services
- Running complex integration tests

**Use `mqtt_topic_factory` when:**
- Testing multiple interacting services
- Need complete environment variable setup
- Want automatic MQTT topic namespacing

**Use both together when:**
- Running complex E2E tests
- Testing complete system integration
- Need maximum test isolation and reliability

## Common Environment Variables

### Core Settings
- `CONSENSUS_THRESHOLD=2` - Cameras required for fire trigger
- `MIN_CONFIDENCE=0.7` - AI detection confidence threshold
- `MAX_ENGINE_RUNTIME=1800` - Safety limit in seconds
- `FRIGATE_DETECTOR=auto` - AI accelerator selection

### Development Settings
- `LOG_LEVEL=DEBUG` - Verbose logging
- `GPIO_SIMULATION=true` - Safe testing without hardware
- `DISCOVERY_INTERVAL=300` - Camera discovery frequency

## Deployment Considerations

### Balena Cloud
- Fleet management for multiple edge devices
- Environment variables managed via Balena dashboard
- Automatic updates and rollback support

### Resource Requirements
- Minimum 4GB RAM for Frigate + AI detection
- 32GB+ storage for video retention
- USB 3.0 for Coral TPU, PCIe for Hailo
- GPIO access required for pump control

### Network Architecture
- Cameras discovered via ONVIF, mDNS, port scanning
- MQTT broker creates isolated network (192.168.100.0/24)
- Frigate UI accessible on port 5000
- TLS encryption for production MQTT (port 8883)

## File Organization Guidelines

### Directory Structure
- **tmp/** - Temporary test scripts, debugging files, intermediate outputs
- **output/** - Final test results, reports, converted models
- **scripts/** - Permanent utility scripts (kept in repository)
- **docs/** - Documentation files and guides
- **converted_models/** - Model conversion scripts and utilities
- **tests/** - Test files (must start with `test_` prefix)

### Examples:
```bash
# Temporary files
tmp/test_tensorrt_fix.py
tmp/debug_yolov9.py

# Output files
output/test_results.log
output/model_conversion_report.md

# Permanent scripts
scripts/validate_models.py
scripts/generate_certs.sh
```

## Development Workflow for Non-Trivial Work

For any work >30 minutes or involving multiple files:

1. **Create a Plan File**
   - Name: `[feature_name]_plan.md` in appropriate directory
   - Include: Overview, phases, timeline, technical requirements

2. **Execute Plan with Progress Updates**
   - Mark phases: `## Phase X: [Name] - ⏳ IN PROGRESS` → `✅ COMPLETE`
   - Document deviations or issues encountered

3. **Testing Requirements**
   - Run all tests related to changed code
   - Fix the program's code first, not the test
   - Document skipped tests with specific reasons

### Plan File Template
```markdown
# [Feature Name] Implementation Plan

## Overview
Brief description of what will be accomplished

## Phases
### Phase 1: [Name] - ⏳ PENDING
- Task 1
- Task 2

## Testing
- List of test files that will be affected
- Expected test changes

## Progress Notes
[Add progress updates here as work proceeds]

## Test Results
- Tests run: X
- Tests passed: Y
- Tests failed: Z
- Tests skipped: N (with reasons)
```

## Documentation Best Practices

### Sphinx-Compatible Documentation
All Python code should follow Sphinx documentation standards. Use Google-style docstrings.

#### Documentation Principles
1. **Component Connectivity**: Document MQTT topic connections
2. **Parameter Implications**: Document non-obvious effects
3. **Side Effects**: Document MQTT publishes
4. **Error Handling**: Document exceptions
5. **Thread Safety**: Document synchronization requirements
6. **MQTT Topics**: Document all pub/sub topics

## AI Assistant Guidelines

### MCP Tool Usage for Enhanced Development

#### Critical Safety and Security Analysis
For a safety-critical fire suppression system, use specialized tools:

- **`mcp__zen__secaudit`** - Comprehensive security audit for OWASP analysis, vulnerability assessment
  - Use for GPIO control code, MQTT authentication, certificate management
  - Choose `pro` model for deep security analysis
  - Focus on hardware control safety and network security

- **`mcp__zen__codereview`** - Professional code review for bugs, security, performance
  - Essential for pump control logic, consensus algorithms, camera detection
  - Use for safety-critical paths and hardware interfaces
  - Identifies subtle issues that could cause system failures

#### Development Workflow Tools
Follow the recommended workflow: **Design → Review → Implement → Test → Precommit**

- **`mcp__zen__planner`** - Interactive step-by-step planning for complex features
  - Use for new AI model integrations, multi-service features
  - Break down complex tasks like Hailo integration or Coral TPU support

- **`mcp__zen__debug`** - Systematic investigation and root cause analysis
  - Use when encountering specific errors or mysterious behaviors
  - Provides step-by-step investigation workflow

- **`mcp__zen__analyze`** - Architecture and code pattern analysis
  - Use to understand microservices communication patterns
  - Analyze MQTT message flows and service dependencies

- **`mcp__zen__testgen`** - Comprehensive test generation with edge cases
  - Create test suites for hardware interfaces, consensus logic
  - Generate tests for edge cases in fire detection algorithms

- **`mcp__zen__precommit`** - Pre-commit validation of git changes
  - Validate changes before commits, especially for safety-critical code
  - Ensure test coverage and documentation updates

#### Collaborative Analysis and Decision Making

- **`mcp__zen__challenge`** - Critical challenge prompt to question assumptions
  - Use before implementing safety-critical features
  - Challenge design decisions for pump control, fire detection thresholds
  - Prevents automatic agreement, encourages thorough analysis

- **`mcp__zen__consensus`** - Multi-model perspective gathering
  - Get diverse expert opinions on architectural decisions
  - Use for complex design choices like consensus algorithms
  - Combine different AI models for comprehensive analysis

- **`mcp__zen__chat`** - Collaborative thinking and brainstorming
  - Bounce ideas for new features, optimization strategies
  - Get second opinions on technical approaches
  - Use for general development discussions

#### Model Selection Strategy
Choose models based on task complexity and domain:

- **Gemini Pro**: Large context analysis, architecture decisions, security reviews
- **o3**: Logic problems, algorithmic optimization, mathematical reasoning
- **Enable web search**: For current API documentation, best practices, error solutions

#### Plan Review Recommendations
**For critical features, review implementation plans with advanced models:**

- **Use o3 or Gemini Pro** to review plans created by other models
- **Critical for safety systems**: Fire suppression, GPIO control, consensus algorithms
- **Review focus areas**:
  - Technical feasibility and approach
  - Safety considerations and edge cases
  - Resource requirements and timelines
  - Integration points and dependencies
  - Testing and validation strategies
- **Implementation**: Use `mcp__zen__consensus` with o3 and Gemini as reviewers

#### Tool Selection Decision Flow
1. **Specific error or bug?** → `mcp__zen__debug`
2. **Want to find code issues?** → `mcp__zen__codereview`
3. **Need security analysis?** → `mcp__zen__secaudit`
4. **Understand code architecture?** → `mcp__zen__analyze`
5. **Need comprehensive tests?** → `mcp__zen__testgen`
6. **Complex feature planning?** → `mcp__zen__planner`
7. **Want to challenge assumptions?** → `mcp__zen__challenge`
8. **Need multiple perspectives?** → `mcp__zen__consensus`
9. **Ready to commit changes?** → `mcp__zen__precommit`

### Traditional Model Usage Guidelines

#### Debugging and Code Analysis
**Use Gemini for:**
- Large context analysis (>10 files or >5000 lines)
- Complex debugging across multiple files
- Architecture analysis
- Performance analysis across components

**Use o3 for:**
- Tricky logic problems
- Algorithm optimization
- Single function debugging
- Mathematical reasoning
- Edge case identification

### Information Accuracy Guidelines
**Always use web search for:**
- **All uncertain APIs** - Never guess API parameters, methods, or usage patterns
- Library features and compatibility
- Error message solutions
- Breaking changes between versions

Context7 MCP provides the following tools that LLMs can use:

    resolve-library-id: Resolves a general library name into a Context7-compatible library ID.
        libraryName (required): The name of the library to search for

    get-library-docs: Fetches documentation for a library using a Context7-compatible library ID.
        context7CompatibleLibraryID (required): Exact Context7-compatible library ID (e.g., /mongodb/docs, /vercel/next.js)
        topic (optional): Focus the docs on a specific topic (e.g., "routing", "hooks")
        tokens (optional, default 10000): Max number of tokens to return. Values less than the default value of 10000 are automatically increased to 10000.



**For difficult debugging problems:**
- **Search local .md files first** - Look for similar problems and solutions in project documentation
- Use `Glob` tool to find relevant docs: `*.md`, `docs/*.md`, `*SUMMARY.md`
- Check `*_PLAN.md`, `*_SUMMARY.md`, `*_GUIDE.md` files for past solutions

**Never speculate** - Search and verify all technical details.

## Critical Best Practices Summary

### Best Practices Summary
The following practices are strongly recommended based on proven production improvements:

1. **Base Class Usage**: 78% code reduction achieved - Use base classes for new services
   - Exception: GPIO PumpController uses custom implementation for safety reasons
2. **Service Initialization Order**: Prevents MQTT connection failures - Follow recommended pattern
3. **Integration Testing**: No mocking internal services - prevents false test passes
4. **Docker Management**: Use DockerContainerManager - prevents container conflicts
5. **Model Conversion Caching**: 10-50x speedup - Implement for all conversions

### Consequences of Not Following These Practices
- **Without base classes**: 2000+ lines of duplicate code per service
- **Wrong initialization order**: Services fail to connect to MQTT broker
- **Mocking internal services**: Tests pass but production fails
- **No container management**: Parallel tests fail with name conflicts
- **No model caching**: Tests timeout after 30+ minutes

These practices are the result of extensive debugging and optimization efforts. Following them ensures a stable, maintainable system while allowing for justified exceptions where safety or reliability requires custom implementations.
