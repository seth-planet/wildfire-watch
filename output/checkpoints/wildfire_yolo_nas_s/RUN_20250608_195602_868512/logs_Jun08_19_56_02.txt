[2025-06-08 19:55:04] INFO - super_gradients.common.crash_handler.crash_tips_setup - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it
[2025-06-08 19:55:05] DEBUG - matplotlib - matplotlib data path: /usr/local/lib/python3.10/dist-packages/matplotlib/mpl-data
[2025-06-08 19:55:05] DEBUG - matplotlib - CONFIGDIR=/home/seth/.config/matplotlib
[2025-06-08 19:55:05] DEBUG - matplotlib - interactive is False
[2025-06-08 19:55:05] DEBUG - matplotlib - platform is linux
[2025-06-08 19:55:05] DEBUG - matplotlib - CACHEDIR=/home/seth/.cache/matplotlib
[2025-06-08 19:55:05] DEBUG - matplotlib.font_manager - Using fontManager instance from /home/seth/.cache/matplotlib/fontlist-v390.json
[2025-06-08 19:55:05] DEBUG - super_gradients.common.sg_loggers.clearml_sg_logger - Failed to import clearml
[2025-06-08 19:55:06] DEBUG - hydra.core.utils - Setting JobRuntime:name=UNKNOWN_NAME
[2025-06-08 19:55:06] DEBUG - hydra.core.utils - Setting JobRuntime:name=app
[2025-06-08 19:55:06] DEBUG - hydra.core.utils - Setting JobRuntime:name=app
[2025-06-08 19:55:06] INFO - __main__ - Starting YOLO-NAS training...
[2025-06-08 19:55:06] INFO - __main__ - Using device: None
[2025-06-08 19:55:06] INFO - __main__ - Creating YOLO-NAS dataloaders with real dataset...
[2025-06-08 19:55:06] INFO - __main__ - Creating training dataloader...
[2025-06-08 19:55:06] DEBUG - hydra.core.utils - Setting JobRuntime:name=app
[2025-06-08 19:55:07] INFO - super_gradients.training.datasets.detection_datasets.detection_dataset - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.
[2025-06-08 19:56:00] INFO - __main__ - Creating validation dataloader...
[2025-06-08 19:56:00] DEBUG - hydra.core.utils - Setting JobRuntime:name=app
[2025-06-08 19:56:00] INFO - super_gradients.training.datasets.detection_datasets.detection_dataset - Dataset Initialization in progress. `cache_annotations=True` causes the process to take longer due to full dataset indexing.
[2025-06-08 19:56:02] INFO - __main__ - âœ“ Real dataset dataloaders created successfully
[2025-06-08 19:56:02] INFO - __main__ - Training samples: 283618
[2025-06-08 19:56:02] INFO - __main__ - Validation samples: 21287
[2025-06-08 19:56:02] DEBUG - hydra.core.utils - Setting JobRuntime:name=app
[2025-06-08 19:56:02] INFO - __main__ - Model architecture: YoloNAS_S
[2025-06-08 19:56:02] INFO - __main__ - Number of classes: 32
[2025-06-08 19:56:02] INFO - __main__ - QAT (Quantization Aware Training) enabled
[2025-06-08 19:56:02] INFO - __main__ - QAT will start at epoch 150
[2025-06-08 19:56:02] WARNING - super_gradients.training.sg_trainer.sg_trainer - Train dataset size % batch_size != 0 and drop_last=False, this might result in smaller last batch.
[2025-06-08 19:56:02] INFO - super_gradients.training.sg_trainer.sg_trainer - Starting a new run with `run_id=RUN_20250608_195602_868512`
[2025-06-08 19:56:02] INFO - super_gradients.training.sg_trainer.sg_trainer - Checkpoints directory: ../output/checkpoints/wildfire_yolo_nas_s/RUN_20250608_195602_868512
[2025-06-08 19:56:02] INFO - super_gradients.training.sg_trainer.sg_trainer - Using EMA with params {'decay': 0.9999, 'decay_type': 'threshold'}
[2025-06-08 19:56:03] ERROR - super_gradients.training.sg_trainer.sg_trainer - Uncaught exception
Traceback (most recent call last):
  File "/home/seth/wildfire-watch/output/../output/run_training.py", line 210, in <module>
    trained_model_path = main()
  File "/home/seth/wildfire-watch/output/../output/run_training.py", line 187, in main
    trainer.train(
  File "/home/seth/.local/lib/python3.10/site-packages/super_gradients/training/sg_trainer/sg_trainer.py", line 1480, in train
    first_train_batch = next(iter(self.train_loader))
  File "/home/seth/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 733, in __next__
    data = self._next_data()
  File "/home/seth/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1515, in _next_data
    return self._process_data(data, worker_id)
  File "/home/seth/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1550, in _process_data
    data.reraise()
  File "/home/seth/.local/lib/python3.10/site-packages/torch/_utils.py", line 750, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/seth/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/seth/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/seth/.local/lib/python3.10/site-packages/super_gradients/training/utils/collate_fn/detection_collate_fn.py", line 25, in __call__
    return self._format_images(images_batch), self._format_targets(labels_batch)
  File "/home/seth/.local/lib/python3.10/site-packages/super_gradients/training/utils/collate_fn/detection_collate_fn.py", line 30, in _format_images
    images_batch_stack = torch.stack(images_batch, 0)
RuntimeError: stack expects each tensor to be equal size, but got [450, 640, 3] at entry 0 and [428, 640, 3] at entry 1

